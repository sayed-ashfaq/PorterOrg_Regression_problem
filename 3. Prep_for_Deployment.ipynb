{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a940da-79a8-4003-b6c6-7631ddcae24e",
   "metadata": {},
   "source": [
    "## Module 10 -- Deployment -- Recap "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109b9fd-4274-4c6b-97bd-17041e666b36",
   "metadata": {},
   "source": [
    "#### How to think before thinking of deploying - There're Several Critical Steps\n",
    "\n",
    "You got the broad strokes but a production pipeline needs to be exact. Missing one transformation = silent garbage predictions. No errors, just wrong numbers.\n",
    "> List the inference transformation steps in the correct order that raw input data must go through before hitting `model.predict()`.\n",
    "```\n",
    "Let me give you a hint. A customer places an order right now. We receive this JSON:\n",
    "{\n",
    "  \"created_at\": \"2025-02-20 19:30:00\",\n",
    "  \"market_id\": 3,\n",
    "  \"store_primary_category\": \"italian\",\n",
    "  \"order_protocol\": 1,\n",
    "  \"total_items\": 3,\n",
    "  \"subtotal\": 2500,\n",
    "  \"num_distinct_items\": 2,\n",
    "  \"min_item_price\": 600,\n",
    "  \"max_item_price\": 1200,\n",
    "  \"total_onshift_partners\": 12,\n",
    "  \"total_busy_partners\": 8,\n",
    "  \"total_outstanding_orders\": 15\n",
    "} \n",
    "```\n",
    ">  what happens to a single raw order JSON at prediction time. The sequence of data transformations.\n",
    "\n",
    "Here's the complete correct order:\n",
    "\n",
    "```\n",
    "1. Parse created_at → extract hour, day_of_week, month\n",
    "2. Engineer is_peak from hour\n",
    "3. Engineer time_period → encode using TRAINING category means\n",
    "4. Engineer demand_supply_ratio = outstanding_orders / (onshift + 1)\n",
    "5. Engineer price_range = max_item_price - min_item_price\n",
    "6. Engineer avg_item_price = subtotal / total_items\n",
    "7. Encode store_primary_category → training means (or global mean if unseen)\n",
    "8. One-hot encode market_id and order_protocol (drop_first=True)\n",
    "9. Drop store_id, created_at, raw datetime cols\n",
    "10. Apply log1p to: total_items, subtotal, min_item_price, \n",
    "                    max_item_price, total_outstanding_orders\n",
    "11. scaler.transform() — using saved scaler.pkl\n",
    "12. model.predict()\n",
    "13. np.expm1() → real minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3dc3f2-bb0a-40ac-9150-0987eec257eb",
   "metadata": {},
   "source": [
    "### Build the Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9c19bf-6536-4211-9733-30021a0376f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from typing import Dict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68250f5a-f576-45a2-b20f-9361969d00aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:\n",
      "  Predicted: 120.0 minutes\n",
      "  Range: 110.0 - 120.0 minutes\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(raw_input: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform raw order input through exact same pipeline as training.\n",
    "    Input:  raw order dictionary\n",
    "    Output: scaled numpy array ready for model.predict()\n",
    "    \"\"\"\n",
    "    # Load artifacts\n",
    "    scaler = joblib.load('porter_model/scaler.pkl')\n",
    "    category_map = joblib.load('porter_model/category_encoding.pkl')\n",
    "    time_period_map = joblib.load('porter_model/time_period_encoding.pkl')\n",
    "    global_mean = joblib.load('porter_model/global_mean.pkl')\n",
    "    feature_cols = joblib.load('porter_model/feature_columns.pkl')\n",
    "    \n",
    "    # Work on a copy\n",
    "    data = raw_input.copy()\n",
    "    \n",
    "    # ── Step 1: Parse datetime ─────────────────────────────────────────\n",
    "    created_at = pd.to_datetime(data['created_at'])\n",
    "    data['hour'] = created_at.hour\n",
    "    data['day_of_week'] = created_at.dayofweek\n",
    "    data['month'] = created_at.month\n",
    "    \n",
    "    # ── Step 2: Time features ──────────────────────────────────────────\n",
    "    def get_time_period(hour):\n",
    "        if 6 <= hour <= 9: return 'breakfast'\n",
    "        elif 11 <= hour <= 14: return 'lunch'\n",
    "        elif 17 <= hour <= 21: return 'dinner'\n",
    "        elif hour in [22, 23, 0, 1]: return 'late_night'\n",
    "        else: return 'off_peak'\n",
    "    \n",
    "    data['is_peak'] = 1 if data['hour'] in [\n",
    "        11,12,13,14,15,19,20,21] else 0\n",
    "    time_period = get_time_period(data['hour'])\n",
    "    data['time_period_encoded'] = time_period_map.get(\n",
    "        time_period, global_mean)\n",
    "    data['is_weekend'] = 1 if data['day_of_week'] >= 5 else 0\n",
    "    \n",
    "    # ── Step 3: Category encoding ──────────────────────────────────────\n",
    "    category = data.get('store_primary_category', 'unknown')\n",
    "    data['category_encoded'] = category_map.get(\n",
    "        category, global_mean)\n",
    "    \n",
    "    # ── Step 4: Supply demand features ────────────────────────────────\n",
    "    data['demand_supply_ratio'] = (\n",
    "        data['total_outstanding_orders'] / \n",
    "        (data['total_onshift_partners'] + 1)\n",
    "    )\n",
    "    data['price_range'] = (\n",
    "        data['max_item_price'] - data['min_item_price'])\n",
    "    data['avg_item_price'] = (\n",
    "        data['subtotal'] / data['total_items'])\n",
    "    \n",
    "    # ── Step 5: Log1p transformations ─────────────────────────────────\n",
    "    for col in ['total_items', 'subtotal', 'min_item_price',\n",
    "                'max_item_price', 'total_outstanding_orders']:\n",
    "        data[col] = np.log1p(max(data[col], 0))\n",
    "    \n",
    "    # ── Step 6: One-hot encode market_id and order_protocol ───────────\n",
    "    for i in [2, 3, 4, 5, 6]:\n",
    "        data[f'market_id_{i}.0'] = 1 if data['market_id'] == i else 0\n",
    "    for i in [2, 3, 4, 5, 6, 7]:\n",
    "        data[f'order_protocol_{i}.0'] = (\n",
    "            1 if data['order_protocol'] == i else 0)\n",
    "    \n",
    "    # ── Step 7: Build dataframe in exact training column order ─────────\n",
    "    df_input = pd.DataFrame([data])\n",
    "    df_input = df_input[feature_cols]\n",
    "    \n",
    "    # ── Step 8: Scale ──────────────────────────────────────────────────\n",
    "    df_scaled = scaler.transform(df_input)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def predict_delivery_time(raw_input: dict) -> dict:\n",
    "    \"\"\"\n",
    "    End to end prediction function.\n",
    "    Returns prediction in real minutes with confidence context.\n",
    "    \"\"\"\n",
    "    model = keras.models.load_model(\n",
    "        'porter_model/best_model_v2.keras')\n",
    "    \n",
    "    # Preprocess\n",
    "    X = preprocess_input(raw_input)\n",
    "    \n",
    "    # Predict in log space\n",
    "    y_log = model.predict(X, verbose=0).flatten()[0]\n",
    "    \n",
    "    # Inverse transform to real minutes\n",
    "    y_minutes = float(np.expm1(y_log))\n",
    "    \n",
    "    # Clamp to realistic range\n",
    "    y_minutes = max(5.0, min(120.0, y_minutes))\n",
    "    \n",
    "    return {\n",
    "        'predicted_delivery_minutes': round(y_minutes, 1),\n",
    "        'predicted_delivery_range': {\n",
    "            'optimistic': round(max(5.0, y_minutes - 10), 1),\n",
    "            'pessimistic': round(min(120.0, y_minutes + 10), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ── Test the pipeline ──────────────────────────────────────────────────\n",
    "test_order = {\n",
    "    \"created_at\": \"2015-02-15 19:30:00\",\n",
    "    \"market_id\": 3,\n",
    "    \"store_primary_category\": \"italian\",\n",
    "    \"order_protocol\": 1,\n",
    "    \"total_items\": 3,\n",
    "    \"subtotal\": 2500,\n",
    "    \"num_distinct_items\": 2,\n",
    "    \"min_item_price\": 600,\n",
    "    \"max_item_price\": 1200,\n",
    "    \"total_onshift_partners\": 12,\n",
    "    \"total_busy_partners\": 8,\n",
    "    \"total_outstanding_orders\": 15\n",
    "}\n",
    "\n",
    "result = predict_delivery_time(test_order)\n",
    "print(\"Test prediction:\")\n",
    "print(f\"  Predicted: {result['predicted_delivery_minutes']} minutes\")\n",
    "print(f\"  Range: {result['predicted_delivery_range']['optimistic']}\"\n",
    "      f\" - {result['predicted_delivery_range']['pessimistic']} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48af88-3010-47a2-9c58-a305b9a8a25e",
   "metadata": {},
   "source": [
    "#### The Pipeline Ran But the Prediction Is Wrong\n",
    "120 minutes is our clamping ceiling — the model predicted something above 120 and we clamped it down. That means the raw prediction was unrealistic. Something in the preprocessing went wrong.\n",
    "\n",
    "Let's debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e211a5ba-fdfd-4ec6-af6e-8bcd6a6bdca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — Available columns:\n",
      "['avg_item_price', 'category_encoded', 'created_at', 'day_of_week', 'demand_supply_ratio', 'hour', 'is_peak', 'is_weekend', 'market_id', 'market_id_2.0', 'market_id_3.0', 'market_id_4.0', 'market_id_5.0', 'market_id_6.0', 'max_item_price', 'min_item_price', 'month', 'num_distinct_items', 'order_protocol', 'order_protocol_2.0', 'order_protocol_3.0', 'order_protocol_4.0', 'order_protocol_5.0', 'order_protocol_6.0', 'order_protocol_7.0', 'price_range', 'store_primary_category', 'subtotal', 'time_period_encoded', 'total_busy_partners', 'total_items', 'total_onshift_partners', 'total_outstanding_orders']\n",
      "\n",
      "DEBUG — Expected columns:\n",
      "['avg_item_price', 'category_encoded', 'day_of_week', 'demand_supply_ratio', 'hour', 'is_weekend', 'market_id_2.0', 'market_id_3.0', 'market_id_4.0', 'market_id_5.0', 'market_id_6.0', 'max_item_price', 'min_item_price', 'month', 'num_distinct_items', 'order_protocol_2.0', 'order_protocol_3.0', 'order_protocol_4.0', 'order_protocol_5.0', 'order_protocol_6.0', 'order_protocol_7.0', 'price_range', 'subtotal', 'time_period_encoded', 'total_items']\n",
      "\n",
      "DEBUG — Missing columns:\n",
      "set()\n",
      "\n",
      "DEBUG — Extra columns:\n",
      "{'total_onshift_partners', 'store_primary_category', 'order_protocol', 'market_id', 'is_peak', 'total_busy_partners', 'total_outstanding_orders', 'created_at'}\n",
      "\n",
      "DEBUG — Feature values before scaling:\n",
      "                              0\n",
      "total_items            1.386294\n",
      "subtotal               7.824446\n",
      "num_distinct_items     2.000000\n",
      "min_item_price         6.398595\n",
      "max_item_price         7.090910\n",
      "hour                  19.000000\n",
      "day_of_week            6.000000\n",
      "month                  2.000000\n",
      "market_id_2.0          0.000000\n",
      "market_id_3.0          1.000000\n",
      "market_id_4.0          0.000000\n",
      "market_id_5.0          0.000000\n",
      "market_id_6.0          0.000000\n",
      "order_protocol_2.0     0.000000\n",
      "order_protocol_3.0     0.000000\n",
      "order_protocol_4.0     0.000000\n",
      "order_protocol_5.0     0.000000\n",
      "order_protocol_6.0     0.000000\n",
      "order_protocol_7.0     0.000000\n",
      "category_encoded       3.823752\n",
      "demand_supply_ratio    1.153846\n",
      "price_range          600.000000\n",
      "avg_item_price       833.333333\n",
      "time_period_encoded    3.725902\n",
      "is_weekend             1.000000\n",
      "\n",
      "DEBUG — Raw log prediction: 7.0180\n",
      "DEBUG — Raw minutes prediction: 1115.52\n"
     ]
    }
   ],
   "source": [
    "def predict_delivery_time_debug(raw_input: dict) -> dict:\n",
    "    \n",
    "    scaler = joblib.load('porter_model/scaler.pkl')\n",
    "    category_map = joblib.load('porter_model/category_encoding.pkl')\n",
    "    time_period_map = joblib.load('porter_model/time_period_encoding.pkl')\n",
    "    global_mean = joblib.load('porter_model/global_mean.pkl')\n",
    "    feature_cols = joblib.load('porter_model/feature_columns.pkl')\n",
    "    model = keras.models.load_model('porter_model/best_model_v2.keras')\n",
    "    \n",
    "    data = raw_input.copy()\n",
    "    \n",
    "    # Parse datetime\n",
    "    created_at = pd.to_datetime(data['created_at'])\n",
    "    data['hour'] = created_at.hour\n",
    "    data['day_of_week'] = created_at.dayofweek\n",
    "    data['month'] = created_at.month\n",
    "    \n",
    "    def get_time_period(hour):\n",
    "        if 6 <= hour <= 9: return 'breakfast'\n",
    "        elif 11 <= hour <= 14: return 'lunch'\n",
    "        elif 17 <= hour <= 21: return 'dinner'\n",
    "        elif hour in [22, 23, 0, 1]: return 'late_night'\n",
    "        else: return 'off_peak'\n",
    "    \n",
    "    data['is_peak'] = 1 if data['hour'] in [\n",
    "        11,12,13,14,15,19,20,21] else 0\n",
    "    time_period = get_time_period(data['hour'])\n",
    "    data['time_period_encoded'] = time_period_map.get(\n",
    "        time_period, global_mean)\n",
    "    data['is_weekend'] = 1 if data['day_of_week'] >= 5 else 0\n",
    "    \n",
    "    category = data.get('store_primary_category', 'unknown')\n",
    "    data['category_encoded'] = category_map.get(\n",
    "        category, global_mean)\n",
    "    \n",
    "    data['demand_supply_ratio'] = (\n",
    "        data['total_outstanding_orders'] / \n",
    "        (data['total_onshift_partners'] + 1))\n",
    "    data['price_range'] = (\n",
    "        data['max_item_price'] - data['min_item_price'])\n",
    "    data['avg_item_price'] = (\n",
    "        data['subtotal'] / data['total_items'])\n",
    "    \n",
    "    # Log transform\n",
    "    for col in ['total_items', 'subtotal', 'min_item_price',\n",
    "                'max_item_price', 'total_outstanding_orders']:\n",
    "        data[col] = np.log1p(max(data[col], 0))\n",
    "    \n",
    "    # One hot encode\n",
    "    for i in [2, 3, 4, 5, 6]:\n",
    "        data[f'market_id_{i}.0'] = 1 if data['market_id'] == i else 0\n",
    "    for i in [2, 3, 4, 5, 6, 7]:\n",
    "        data[f'order_protocol_{i}.0'] = (\n",
    "            1 if data['order_protocol'] == i else 0)\n",
    "    \n",
    "    # Build dataframe\n",
    "    df_input = pd.DataFrame([data])\n",
    "    \n",
    "    print(\"DEBUG — Available columns:\")\n",
    "    print(sorted(df_input.columns.tolist()))\n",
    "    print(f\"\\nDEBUG — Expected columns:\")\n",
    "    print(sorted(feature_cols))\n",
    "    print(f\"\\nDEBUG — Missing columns:\")\n",
    "    missing = set(feature_cols) - set(df_input.columns)\n",
    "    print(missing)\n",
    "    print(f\"\\nDEBUG — Extra columns:\")\n",
    "    extra = set(df_input.columns) - set(feature_cols)\n",
    "    print(extra)\n",
    "    \n",
    "    # Select correct columns\n",
    "    df_input = df_input[feature_cols]\n",
    "    \n",
    "    print(f\"\\nDEBUG — Feature values before scaling:\")\n",
    "    print(df_input.T)\n",
    "    \n",
    "    # Scale\n",
    "    df_scaled = scaler.transform(df_input)\n",
    "    \n",
    "    # Predict\n",
    "    y_log = model.predict(df_scaled, verbose=0).flatten()[0]\n",
    "    y_minutes = float(np.expm1(y_log))\n",
    "    \n",
    "    print(f\"\\nDEBUG — Raw log prediction: {y_log:.4f}\")\n",
    "    print(f\"DEBUG — Raw minutes prediction: {y_minutes:.2f}\")\n",
    "    \n",
    "    return y_minutes\n",
    "\n",
    "raw_result = predict_delivery_time_debug(test_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511989c-8842-4864-8f36-99f134e670cc",
   "metadata": {},
   "source": [
    "### The Lesson Here\n",
    "The order of operations in inference must exactly mirror training. In training we log transformed features first, then engineered price_range and avg_item_price from already-transformed values. In our first inference attempt we reversed the order. The scaler had never seen raw 600 — it expected log-scale ~0.69.\n",
    "\n",
    "This is the most common deployment bug in production ML systems. The model is perfect. The pipeline order was wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364d8bf5-de75-45bb-8e69-d261317f9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature values after fix:\n",
      "                             0\n",
      "total_items           1.386294\n",
      "subtotal              7.824446\n",
      "num_distinct_items    2.000000\n",
      "min_item_price        6.398595\n",
      "max_item_price        7.090910\n",
      "hour                 19.000000\n",
      "day_of_week           6.000000\n",
      "month                 2.000000\n",
      "market_id_2.0         0.000000\n",
      "market_id_3.0         1.000000\n",
      "market_id_4.0         0.000000\n",
      "market_id_5.0         0.000000\n",
      "market_id_6.0         0.000000\n",
      "order_protocol_2.0    0.000000\n",
      "order_protocol_3.0    0.000000\n",
      "order_protocol_4.0    0.000000\n",
      "order_protocol_5.0    0.000000\n",
      "order_protocol_6.0    0.000000\n",
      "order_protocol_7.0    0.000000\n",
      "category_encoded      3.823752\n",
      "demand_supply_ratio   1.153846\n",
      "price_range           0.692315\n",
      "avg_item_price        5.644145\n",
      "time_period_encoded   3.725902\n",
      "is_weekend            1.000000\n",
      "\n",
      "Predicted: 37.3 minutes\n",
      "Range: {'optimistic': 27.3, 'pessimistic': 47.3}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input_fixed(raw_input: dict) -> np.ndarray:\n",
    "    \n",
    "    scaler = joblib.load('porter_model/scaler.pkl')\n",
    "    category_map = joblib.load('porter_model/category_encoding.pkl')\n",
    "    time_period_map = joblib.load('porter_model/time_period_encoding.pkl')\n",
    "    global_mean = joblib.load('porter_model/global_mean.pkl')\n",
    "    feature_cols = joblib.load('porter_model/feature_columns.pkl')\n",
    "    \n",
    "    data = raw_input.copy()\n",
    "    \n",
    "    # ── Step 1: DateTime features ──────────────────────────────────────\n",
    "    created_at = pd.to_datetime(data['created_at'])\n",
    "    data['hour'] = created_at.hour\n",
    "    data['day_of_week'] = created_at.dayofweek\n",
    "    data['month'] = created_at.month\n",
    "    \n",
    "    # ── Step 2: Time period features ──────────────────────────────────\n",
    "    def get_time_period(hour):\n",
    "        if 6 <= hour <= 9: return 'breakfast'\n",
    "        elif 11 <= hour <= 14: return 'lunch'\n",
    "        elif 17 <= hour <= 21: return 'dinner'\n",
    "        elif hour in [22, 23, 0, 1]: return 'late_night'\n",
    "        else: return 'off_peak'\n",
    "    \n",
    "    data['is_weekend'] = 1 if data['day_of_week'] >= 5 else 0\n",
    "    time_period = get_time_period(data['hour'])\n",
    "    data['time_period_encoded'] = time_period_map.get(\n",
    "        time_period, global_mean)\n",
    "    \n",
    "    # ── Step 3: Category encoding ──────────────────────────────────────\n",
    "    category = data.get('store_primary_category', 'unknown')\n",
    "    data['category_encoded'] = category_map.get(\n",
    "        category, global_mean)\n",
    "    \n",
    "    # ── Step 4: Log1p FIRST before any feature engineering ────────────\n",
    "    # This must happen before price_range and avg_item_price\n",
    "    for col in ['total_items', 'subtotal', 'min_item_price',\n",
    "                'max_item_price', 'total_outstanding_orders']:\n",
    "        data[col] = np.log1p(max(data[col], 0))\n",
    "    \n",
    "    # ── Step 5: Engineer features AFTER log transform ─────────────────\n",
    "    # Now price_range uses log-scale prices — matches training\n",
    "    data['price_range'] = (\n",
    "        data['max_item_price'] - data['min_item_price'])\n",
    "    \n",
    "    # avg_item_price uses log subtotal / log total_items\n",
    "    data['avg_item_price'] = (\n",
    "        data['subtotal'] / max(data['total_items'], 0.0001))\n",
    "    \n",
    "    # demand_supply_ratio uses raw outstanding orders\n",
    "    # reverse log1p first with expm1\n",
    "    data['demand_supply_ratio'] = (\n",
    "        np.expm1(data['total_outstanding_orders']) / \n",
    "        (raw_input['total_onshift_partners'] + 1))\n",
    "    \n",
    "    # ── Step 6: One-hot encoding ───────────────────────────────────────\n",
    "    for i in [2, 3, 4, 5, 6]:\n",
    "        data[f'market_id_{i}.0'] = (\n",
    "            1 if raw_input['market_id'] == i else 0)\n",
    "    for i in [2, 3, 4, 5, 6, 7]:\n",
    "        data[f'order_protocol_{i}.0'] = (\n",
    "            1 if raw_input['order_protocol'] == i else 0)\n",
    "    \n",
    "    # ── Step 7: Build dataframe in exact column order ──────────────────\n",
    "    df_input = pd.DataFrame([data])\n",
    "    df_input = df_input[feature_cols]\n",
    "    \n",
    "    print(f\"Feature values after fix:\")\n",
    "    print(df_input.T)\n",
    "    \n",
    "    # ── Step 8: Scale ──────────────────────────────────────────────────\n",
    "    df_scaled = scaler.transform(df_input)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def predict_delivery_time_fixed(raw_input: dict) -> dict:\n",
    "    model = keras.models.load_model(\n",
    "        'porter_model/best_model_v2.keras')\n",
    "    \n",
    "    X = preprocess_input_fixed(raw_input)\n",
    "    y_log = model.predict(X, verbose=0).flatten()[0]\n",
    "    y_minutes = float(np.expm1(y_log))\n",
    "    y_minutes = max(5.0, min(120.0, y_minutes))\n",
    "    \n",
    "    return {\n",
    "        'predicted_delivery_minutes': round(y_minutes, 1),\n",
    "        'predicted_delivery_range': {\n",
    "            'optimistic': round(max(5.0, y_minutes - 10), 1),\n",
    "            'pessimistic': round(min(120.0, y_minutes + 10), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "result = predict_delivery_time_fixed(test_order)\n",
    "print(f\"\\nPredicted: {result['predicted_delivery_minutes']} minutes\")\n",
    "print(f\"Range: {result['predicted_delivery_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8273f-b6a8-4555-b648-7b0a24c6982e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
