# Neural Network Notes

## What is Batch Size and it's effects
In neural networks, the training data is often divided into smaller batches, each of which is processed independently before the model parameters are updated. The batch size refers to the number of samples used in each of these smaller batches during training.

* It updates the model parameters after processing small batches of data. 
* Batch size is the number of samples processed before updating neural network weights. It exists to manage memory usage and stabilize optimization by using mini-batch gradient descent instead of full or single-sample updates. Smaller batches introduce gradient noise that can improve generalization but slow training, while larger batches make training faster and smoother but may reduce model robustness. Common practical values are 32â€“128.